{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBIC corpus evaluations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from tabulate import tabulate\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS = [23, 42, 271, 314, 1337]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "SBIC_CATEGORICAL_COLUMNS = [\"groupYN\", \"intentYN\", \"lewdYN\", \"offensiveYN\", \"ingroupYN\"]\n",
    "approaches = [\n",
    "    \"majority\",\n",
    "    \"random\",\n",
    "    \"deberta-v3-base-finetune\",\n",
    "    \"deberta-v3-base-two-task-mtl\",\n",
    "    \"deberta-v3-base-joint-mtl\",\n",
    "    \"deberta-v3-base-joint-mtl-no-aae\",\n",
    "    \"roberta-base-finetune\",\n",
    "    \"roberta-base-two-task-mtl\",\n",
    "    \"roberta-base-joint-mtl\",\n",
    "    \"roberta-base-joint-mtl-no-aae\",\n",
    "    \"bert-base-uncased-finetune\",\n",
    "    \"bert-base-uncased-two-task-mtl\",\n",
    "    \"bert-base-uncased-joint-mtl\",\n",
    "    \"bert-base-uncased-joint-mtl-no-aae\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading AAE annotated SBIC data\n",
    "sbic_train = pd.read_csv(\n",
    "    \"aae-classification/output/sbic-train_aae-annotated-deberta-v3-base-aee-classifier.csv\"\n",
    ")\n",
    "sbic_val = pd.read_csv(\n",
    "    \"aae-classification/output/sbic-val_aae-annotated-deberta-v3-base-aee-classifier.csv\"\n",
    ")\n",
    "sbic_test = pd.read_csv(\n",
    "    \"aae-classification/output/sbic-test_aae-annotated-deberta-v3-base-aee-classifier.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results for STL models\n",
    "finetuned_test = pd.DataFrame()\n",
    "for seed in SEEDS:\n",
    "    results_seed = pd.read_csv(\n",
    "        f\"finetuning/output/sbic-test_predictions-deberta-v3-base-finetune-seed{seed}.csv\"\n",
    "    )\n",
    "    if \"post_id\" not in finetuned_test.columns:\n",
    "        finetuned_test[\"post_id\"] = results_seed[\"post_id\"]\n",
    "\n",
    "    finetuned_test = pd.merge(finetuned_test, results_seed, on=\"post_id\", how=\"left\")\n",
    "\n",
    "finetuned_test_roberta_base = pd.DataFrame()\n",
    "for seed in SEEDS:\n",
    "    results_seed = pd.read_csv(\n",
    "        f\"finetuning/output/sbic-test_predictions-roberta-base-finetune-seed{seed}.csv\"\n",
    "    )\n",
    "    if \"post_id\" not in finetuned_test_roberta_base.columns:\n",
    "        finetuned_test_roberta_base[\"post_id\"] = results_seed[\"post_id\"]\n",
    "\n",
    "    finetuned_test_roberta_base = pd.merge(\n",
    "        finetuned_test_roberta_base, results_seed, on=\"post_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "finetuned_test_bert_base_uncased = pd.DataFrame()\n",
    "for seed in SEEDS:\n",
    "    results_seed = pd.read_csv(\n",
    "        f\"finetuning/output/sbic-test_predictions-bert-base-uncased-finetune-seed{seed}.csv\"\n",
    "    )\n",
    "    if \"post_id\" not in finetuned_test_bert_base_uncased.columns:\n",
    "        finetuned_test_bert_base_uncased[\"post_id\"] = results_seed[\"post_id\"]\n",
    "\n",
    "    finetuned_test_bert_base_uncased = pd.merge(\n",
    "        finetuned_test_bert_base_uncased, results_seed, on=\"post_id\", how=\"left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results for two-task models\n",
    "two_task_mtl_test = pd.DataFrame()\n",
    "for seed in SEEDS:\n",
    "    results_seed = pd.read_csv(\n",
    "        f\"joint-multitask-learning/output/sbic-test_predictions-deberta-v3-base-two-task-mtl-seed{seed}.csv\"\n",
    "    )\n",
    "    if \"post_id\" not in two_task_mtl_test.columns:\n",
    "        two_task_mtl_test[\"post_id\"] = results_seed[\"post_id\"]\n",
    "\n",
    "    two_task_mtl_test = pd.merge(\n",
    "        two_task_mtl_test, results_seed, on=\"post_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "two_task_mtl_test_roberta_base = pd.DataFrame()\n",
    "for seed in SEEDS:\n",
    "    results_seed = pd.read_csv(\n",
    "        f\"joint-multitask-learning/output/sbic-test_predictions-roberta-base-two-task-mtl-seed{seed}.csv\"\n",
    "    )\n",
    "    if \"post_id\" not in two_task_mtl_test_roberta_base.columns:\n",
    "        two_task_mtl_test_roberta_base[\"post_id\"] = results_seed[\"post_id\"]\n",
    "\n",
    "    two_task_mtl_test_roberta_base = pd.merge(\n",
    "        two_task_mtl_test_roberta_base, results_seed, on=\"post_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "two_task_mtl_test_bert_base_uncased = pd.DataFrame()\n",
    "for seed in SEEDS:\n",
    "    results_seed = pd.read_csv(\n",
    "        f\"joint-multitask-learning/output/sbic-test_predictions-bert-base-uncased-two-task-mtl-seed{seed}.csv\"\n",
    "    )\n",
    "    if \"post_id\" not in two_task_mtl_test_bert_base_uncased.columns:\n",
    "        two_task_mtl_test_bert_base_uncased[\"post_id\"] = results_seed[\"post_id\"]\n",
    "\n",
    "    two_task_mtl_test_bert_base_uncased = pd.merge(\n",
    "        two_task_mtl_test_bert_base_uncased, results_seed, on=\"post_id\", how=\"left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results for MTL+AAE models\n",
    "jmtl_test = pd.DataFrame()\n",
    "for seed in SEEDS:\n",
    "    results_seed = pd.read_csv(\n",
    "        f\"joint-multitask-learning/output/sbic-test_predictions-deberta-v3-base-joint-mtl-seed{seed}.csv\"\n",
    "    )\n",
    "    if \"post_id\" not in jmtl_test.columns:\n",
    "        jmtl_test[\"post_id\"] = results_seed[\"post_id\"]\n",
    "\n",
    "    jmtl_test = pd.merge(jmtl_test, results_seed, on=\"post_id\", how=\"left\")\n",
    "\n",
    "jmtl_test_roberta_base = pd.DataFrame()\n",
    "for seed in SEEDS:\n",
    "    results_seed = pd.read_csv(\n",
    "        f\"joint-multitask-learning/output/sbic-test_predictions-roberta-base-joint-mtl-seed{seed}.csv\"\n",
    "    )\n",
    "    if \"post_id\" not in jmtl_test_roberta_base.columns:\n",
    "        jmtl_test_roberta_base[\"post_id\"] = results_seed[\"post_id\"]\n",
    "\n",
    "    jmtl_test_roberta_base = pd.merge(\n",
    "        jmtl_test_roberta_base, results_seed, on=\"post_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "jmtl_test_bert_base_uncased = pd.DataFrame()\n",
    "for seed in SEEDS:\n",
    "    results_seed = pd.read_csv(\n",
    "        f\"joint-multitask-learning/output/sbic-test_predictions-bert-base-uncased-joint-mtl-seed{seed}.csv\"\n",
    "    )\n",
    "    if \"post_id\" not in jmtl_test_bert_base_uncased.columns:\n",
    "        jmtl_test_bert_base_uncased[\"post_id\"] = results_seed[\"post_id\"]\n",
    "\n",
    "    jmtl_test_bert_base_uncased = pd.merge(\n",
    "        jmtl_test_bert_base_uncased, results_seed, on=\"post_id\", how=\"left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results for MTL-no-AAE models\n",
    "jmtl_no_aae_test = pd.DataFrame()\n",
    "for seed in SEEDS:\n",
    "    results_seed = pd.read_csv(\n",
    "        f\"joint-multitask-learning/output/sbic-test_predictions-deberta-v3-base-joint-mtl-no-aae-seed{seed}.csv\"\n",
    "    )\n",
    "    if \"post_id\" not in jmtl_no_aae_test.columns:\n",
    "        jmtl_no_aae_test[\"post_id\"] = results_seed[\"post_id\"]\n",
    "\n",
    "    jmtl_no_aae_test = pd.merge(\n",
    "        jmtl_no_aae_test, results_seed, on=\"post_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "jmtl_no_aae_test_roberta_base = pd.DataFrame()\n",
    "for seed in SEEDS:\n",
    "    results_seed = pd.read_csv(\n",
    "        f\"joint-multitask-learning/output/sbic-test_predictions-roberta-base-joint-mtl-no-aae-seed{seed}.csv\"\n",
    "    )\n",
    "    if \"post_id\" not in jmtl_no_aae_test_roberta_base.columns:\n",
    "        jmtl_no_aae_test_roberta_base[\"post_id\"] = results_seed[\"post_id\"]\n",
    "\n",
    "    jmtl_no_aae_test_roberta_base = pd.merge(\n",
    "        jmtl_no_aae_test_roberta_base, results_seed, on=\"post_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "jmtl_no_aae_test_bert_base_uncased = pd.DataFrame()\n",
    "for seed in SEEDS:\n",
    "    results_seed = pd.read_csv(\n",
    "        f\"joint-multitask-learning/output/sbic-test_predictions-bert-base-uncased-joint-mtl-no-aae-seed{seed}.csv\"\n",
    "    )\n",
    "    if \"post_id\" not in jmtl_no_aae_test_bert_base_uncased.columns:\n",
    "        jmtl_no_aae_test_bert_base_uncased[\"post_id\"] = results_seed[\"post_id\"]\n",
    "\n",
    "    jmtl_no_aae_test_bert_base_uncased = pd.merge(\n",
    "        jmtl_no_aae_test_bert_base_uncased, results_seed, on=\"post_id\", how=\"left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority baselines\n",
    "groupYN_majority_label = sbic_test[\"groupYN\"].value_counts(sort=True).index[0]\n",
    "groupYN_majority_predictions = np.full(len(sbic_test), groupYN_majority_label)\n",
    "intentYN_majority_label = sbic_test[\"intentYN\"].value_counts(sort=True).index[0]\n",
    "intentYN_majority_predictions = np.full(len(sbic_test), intentYN_majority_label)\n",
    "lewdYN_majority_label = sbic_test[\"lewdYN\"].value_counts(sort=True).index[0]\n",
    "lewdYN_majority_predictions = np.full(len(sbic_test), lewdYN_majority_label)\n",
    "offensiveYN_majority_label = sbic_test[\"offensiveYN\"].value_counts(sort=True).index[0]\n",
    "offensiveYN_majority_predictions = np.full(len(sbic_test), offensiveYN_majority_label)\n",
    "ingroupYN_majority_label = sbic_test[\"ingroupYN\"].value_counts(sort=True).index[0]\n",
    "ingroupYN_majority_predictions = np.full(len(sbic_test), ingroupYN_majority_label)\n",
    "\n",
    "# Pseudo-random baselines\n",
    "groupYN_random_predictions = np.random.randint(2, size=len(sbic_test))\n",
    "intentYN_random_predictions = np.random.randint(2, size=len(sbic_test))\n",
    "lewdYN_random_predictions = np.random.randint(2, size=len(sbic_test))\n",
    "offensiveYN_random_predictions = np.random.randint(2, size=len(sbic_test))\n",
    "ingroupYN_random_predictions = np.random.randint(2, size=len(sbic_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = sbic_test.copy()\n",
    "\n",
    "# Random baseline predictions\n",
    "predictions_df[\"prediction_groupYN_random\"] = groupYN_random_predictions\n",
    "predictions_df[\"prediction_intentYN_random\"] = intentYN_random_predictions\n",
    "predictions_df[\"prediction_lewdYN_random\"] = lewdYN_random_predictions\n",
    "predictions_df[\"prediction_offensiveYN_random\"] = offensiveYN_random_predictions\n",
    "predictions_df[\"prediction_ingroupYN_random\"] = ingroupYN_random_predictions\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Majority baseline predictions\n",
    "predictions_df[\"prediction_groupYN_majority\"] = groupYN_majority_predictions\n",
    "predictions_df[\"prediction_intentYN_majority\"] = intentYN_majority_predictions\n",
    "predictions_df[\"prediction_lewdYN_majority\"] = lewdYN_majority_predictions\n",
    "predictions_df[\"prediction_offensiveYN_majority\"] = offensiveYN_majority_predictions\n",
    "predictions_df[\"prediction_ingroupYN_majority\"] = ingroupYN_majority_predictions\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Singletask predictions\n",
    "predictions_df = pd.merge(predictions_df, finetuned_test, on=\"post_id\", how=\"left\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Singletask (roberta-base) predictions\n",
    "predictions_df = pd.merge(\n",
    "    predictions_df, finetuned_test_roberta_base, on=\"post_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Singletask (bert-base-uncased) predictions\n",
    "predictions_df = pd.merge(\n",
    "    predictions_df, finetuned_test_bert_base_uncased, on=\"post_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Two-task predictions\n",
    "predictions_df = pd.merge(predictions_df, two_task_mtl_test, on=\"post_id\", how=\"left\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Two-task predictions (roberta-base)\n",
    "predictions_df = pd.merge(\n",
    "    predictions_df, two_task_mtl_test_roberta_base, on=\"post_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Two-task predictions (bert-base-uncased)\n",
    "predictions_df = pd.merge(\n",
    "    predictions_df, two_task_mtl_test_bert_base_uncased, on=\"post_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# MTL predictions\n",
    "predictions_df = pd.merge(predictions_df, jmtl_test, on=\"post_id\", how=\"left\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# MTL predictions (roberta-base)\n",
    "predictions_df = pd.merge(\n",
    "    predictions_df, jmtl_test_roberta_base, on=\"post_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# MTL predictions (bert-base-uncased)\n",
    "predictions_df = pd.merge(\n",
    "    predictions_df, jmtl_test_bert_base_uncased, on=\"post_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# MTL (No AAE) predictions\n",
    "predictions_df = pd.merge(predictions_df, jmtl_no_aae_test, on=\"post_id\", how=\"left\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# MTL (No AAE) predictions (roberta-base)\n",
    "predictions_df = pd.merge(\n",
    "    predictions_df, jmtl_no_aae_test_roberta_base, on=\"post_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# MTL (No AAE) predictions (bert-base-uncased)\n",
    "predictions_df = pd.merge(\n",
    "    predictions_df, jmtl_no_aae_test_bert_base_uncased, on=\"post_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "prediction_columns_by_approach = [\n",
    "    list(filter(lambda x: x if approach in x else None, predictions_df.columns))\n",
    "    for approach in approaches\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class distributions (test set)\")\n",
    "for label in SBIC_CATEGORICAL_COLUMNS:\n",
    "    print(f\"==={label}\")\n",
    "    num_neg = len(sbic_test[sbic_test[label] == 0])\n",
    "    num_pos = len(sbic_test[sbic_test[label] == 1])\n",
    "    print(f\"Negative: {num_neg} ({num_neg / len(sbic_test):.2f})\")\n",
    "    print(\n",
    "        f\"Positive: {num_pos} ({num_pos / len(sbic_test):.2f})\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in SBIC_CATEGORICAL_COLUMNS:\n",
    "    print(\"=\" * 40)\n",
    "    print(label)\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    table_0_headers = [\"Accuracy (0)\", \"Precision (0)\", \"Recall (0)\", \"F1 (0)\"]\n",
    "    table_0 = []\n",
    "    table_1_headers = [\"Accuracy (1)\", \"Precision (1)\", \"Recall (1)\", \"F1 (1)\"]\n",
    "    table_1 = []\n",
    "    table_macro_headers = [\n",
    "        \"Accuracy\",\n",
    "        \"Precision (macro)\",\n",
    "        \"Recall (macro)\",\n",
    "        \"F1 (macro)\",\n",
    "    ]\n",
    "    table_macro = []\n",
    "\n",
    "    for approach_name in approaches:\n",
    "        approach_accuracy_0 = []\n",
    "        approach_precision_0 = []\n",
    "        approach_recall_0 = []\n",
    "        approach_f1_0 = []\n",
    "        approach_accuracy_1 = []\n",
    "        approach_precision_1 = []\n",
    "        approach_recall_1 = []\n",
    "        approach_f1_1 = []\n",
    "        approach_accuracy_macro = []\n",
    "        approach_precision_macro = []\n",
    "        approach_recall_macro = []\n",
    "        approach_f1_macro = []\n",
    "\n",
    "        for seed in SEEDS:\n",
    "            if approach_name == \"majority\" or approach_name == \"random\":\n",
    "                prediction = predictions_df[f\"prediction_{label}_{approach_name}\"]\n",
    "            else:\n",
    "                prediction = predictions_df[\n",
    "                    f\"prediction_{label}_{approach_name}-seed{seed}\"\n",
    "                ]\n",
    "\n",
    "            test_accuracy_0 = accuracy_score(y_true=sbic_test[label], y_pred=prediction)\n",
    "            test_precision_0, test_recall_0, test_f1_0, support_0 = (\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=sbic_test[label],\n",
    "                    y_pred=prediction,\n",
    "                    pos_label=0,\n",
    "                    average=\"binary\",\n",
    "                )\n",
    "            )\n",
    "            approach_accuracy_0.append(test_accuracy_0)\n",
    "            approach_precision_0.append(test_precision_0)\n",
    "            approach_recall_0.append(test_recall_0)\n",
    "            approach_f1_0.append(test_f1_0)\n",
    "            table_0.append(\n",
    "                [\n",
    "                    f\"{approach_name}-seed{seed}\".capitalize(),\n",
    "                    np.round(test_accuracy_0, decimals=3),\n",
    "                    np.round(test_precision_0, decimals=3),\n",
    "                    np.round(test_recall_0, decimals=3),\n",
    "                    np.round(test_f1_0, decimals=3),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            test_accuracy_1 = accuracy_score(y_true=sbic_test[label], y_pred=prediction)\n",
    "            test_precision_1, test_recall_1, test_f1_1, support_1 = (\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=sbic_test[label],\n",
    "                    y_pred=prediction,\n",
    "                    pos_label=1,\n",
    "                    average=\"binary\",\n",
    "                )\n",
    "            )\n",
    "            approach_accuracy_1.append(test_accuracy_1)\n",
    "            approach_precision_1.append(test_precision_1)\n",
    "            approach_recall_1.append(test_recall_1)\n",
    "            approach_f1_1.append(test_f1_1)\n",
    "            table_1.append(\n",
    "                [\n",
    "                    f\"{approach_name}-seed{seed}\".capitalize(),\n",
    "                    np.round(test_accuracy_1, decimals=3),\n",
    "                    np.round(test_precision_1, decimals=3),\n",
    "                    np.round(test_recall_1, decimals=3),\n",
    "                    np.round(test_f1_1, decimals=3),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            test_accuracy_macro = 0\n",
    "            (\n",
    "                test_precision_macro,\n",
    "                test_recall_macro,\n",
    "                test_f1_macro,\n",
    "                support_macro,\n",
    "            ) = precision_recall_fscore_support(\n",
    "                y_true=sbic_test[label],\n",
    "                y_pred=prediction,\n",
    "                average=\"macro\",\n",
    "            )\n",
    "            approach_accuracy_macro.append(test_accuracy_macro)\n",
    "            approach_precision_macro.append(test_precision_macro)\n",
    "            approach_recall_macro.append(test_recall_macro)\n",
    "            approach_f1_macro.append(test_f1_macro)\n",
    "            table_macro.append(\n",
    "                [\n",
    "                    f\"{approach_name}-seed{seed}\".capitalize(),\n",
    "                    test_accuracy_macro,\n",
    "                    np.round(test_precision_macro, decimals=3),\n",
    "                    np.round(test_recall_macro, decimals=3),\n",
    "                    np.round(test_f1_macro, decimals=3),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        table_0.append(\n",
    "            [\n",
    "                f\"{approach_name}-average\".capitalize(),\n",
    "                np.round(np.mean(approach_accuracy_0), decimals=3),\n",
    "                np.round(np.mean(approach_precision_0), decimals=3),\n",
    "                np.round(np.mean(approach_recall_0), decimals=3),\n",
    "                np.round(np.mean(approach_f1_0), decimals=3),\n",
    "            ]\n",
    "        )\n",
    "        table_1.append(\n",
    "            [\n",
    "                f\"{approach_name}-average\".capitalize(),\n",
    "                np.round(np.mean(approach_accuracy_1), decimals=3),\n",
    "                np.round(np.mean(approach_precision_1), decimals=3),\n",
    "                np.round(np.mean(approach_recall_1), decimals=3),\n",
    "                np.round(np.mean(approach_f1_1), decimals=3),\n",
    "            ]\n",
    "        )\n",
    "        table_macro.append(\n",
    "            [\n",
    "                f\"{approach_name}-average\".capitalize(),\n",
    "                np.round(np.mean(approach_accuracy_macro), decimals=3),\n",
    "                np.round(np.mean(approach_precision_macro), decimals=3),\n",
    "                np.round(np.mean(approach_recall_macro), decimals=3),\n",
    "                np.round(np.mean(approach_f1_macro), decimals=3),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    print(\"Negative class binary scores\")\n",
    "    print(tabulate(table_0, headers=table_0_headers, tablefmt=\"rounded_grid\"))\n",
    "    for appr in table_0:\n",
    "        print(f\"{appr[0]}: {appr[1]},,,{appr[2]},,,,{appr[3]},,,,{appr[4]}\")\n",
    "\n",
    "    print(\"Positive class binary scores\")\n",
    "    print(tabulate(table_1, headers=table_1_headers, tablefmt=\"rounded_grid\"))\n",
    "    for appr in table_1:\n",
    "        print(f\"{appr[0]}: ,,{appr[2]},,,,{appr[3]},,,,{appr[4]}\")\n",
    "\n",
    "    print(\"Macro averaged scores\")\n",
    "    print(tabulate(table_macro, headers=table_macro_headers, tablefmt=\"rounded_grid\"))\n",
    "    for appr in table_macro:\n",
    "        print(f\"{appr[0]}: ,,,,{appr[2]},,,,{appr[3]},,,,{appr[4]}\")\n",
    "\n",
    "    # Print full approach table row\n",
    "    print(\"Full table rows\")\n",
    "    for i in range(len(table_0)):\n",
    "        print(\n",
    "            f\"{table_0[i][0]}: \"  # Approach name\n",
    "            f\"{table_0[i][1]},,\"  # Accuracy\n",
    "            f\"{table_1[i][2]},{table_0[i][2]},{table_macro[i][2]},,\"  # Precision\n",
    "            f\"{table_1[i][3]},{table_0[i][3]},{table_macro[i][3]},,\"  # Recall\n",
    "            f\"{table_1[i][4]},{table_0[i][4]},{table_macro[i][4]}\"  # F1\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per dialect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of samples per class per dialect\n",
    "for split_name, split in [\n",
    "    (\"train\", sbic_train),\n",
    "    (\"val\", sbic_val),\n",
    "    (\"test\", sbic_test),\n",
    "]:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Split: {split_name}\")\n",
    "    aae_samples = split[split[\"aae_dialect\"] == 1]\n",
    "    non_aae_samples = split[split[\"aae_dialect\"] == 0]\n",
    "    print(f\"Total AAE:\\t{len(aae_samples)}\")\n",
    "    print(f\"Total Non-AAE:\\t{len(non_aae_samples)}\")\n",
    "\n",
    "    # Number of samples per class\n",
    "    for label in SBIC_CATEGORICAL_COLUMNS:\n",
    "        print(\"-\" * 40)\n",
    "        print(label)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        print(f\"AAE, positive:\\t\\t{len(aae_samples[aae_samples[label] == 1])}\")\n",
    "        print(f\"AAE, negative:\\t\\t{len(aae_samples[aae_samples[label] == 0])}\")\n",
    "        print(\n",
    "            f\"Non-AAE, positive:\\t{len(non_aae_samples[non_aae_samples[label] == 1])}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Non-AAE, negative:\\t{len(non_aae_samples[non_aae_samples[label] == 0])}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_counts(y_true: List[int], y_pred: List[int], pos_label: int = 1):\n",
    "    \"\"\"Only works for binary classification\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    neg_label = 0 if pos_label == 1 else 1\n",
    "\n",
    "    tp = np.sum(np.logical_and(y_pred == pos_label, y_true == pos_label))\n",
    "    tn = np.sum(np.logical_and(y_pred == neg_label, y_true == neg_label))\n",
    "    fp = np.sum(np.logical_and(y_pred == pos_label, y_true == neg_label))\n",
    "    fn = np.sum(np.logical_and(y_pred == neg_label, y_true == pos_label))\n",
    "\n",
    "    return (tp, tn, fp, fn)\n",
    "\n",
    "\n",
    "def true_positive_rate(\n",
    "    y_true: List[int], y_pred: List[int], average: str = \"binary\", pos_label: int = 1\n",
    ") -> float:\n",
    "    # implemented as tp/(tp+fn) for a specific group\n",
    "    # tp, tn, fp, fn = get_instance_counts(y_true, y_pred)\n",
    "    # return tp / (tp + fn)\n",
    "    return recall_score(\n",
    "        y_true=y_true, y_pred=y_pred, average=average, pos_label=pos_label\n",
    "    )\n",
    "\n",
    "\n",
    "def false_positive_rate(\n",
    "    y_true: List[int], y_pred: List[int], average: str = \"binary\", pos_label: int = 1\n",
    ") -> float:\n",
    "    # implemented as fp/(fp+tn) for a specific group\n",
    "    if average == \"binary\":\n",
    "        tp, tn, fp, fn = get_instance_counts(y_true, y_pred, pos_label=pos_label)\n",
    "        return fp / (fp + tn)\n",
    "    elif average == \"macro\":\n",
    "        tp_0, tn_0, fp_0, fn_0 = get_instance_counts(y_true, y_pred, pos_label=0)\n",
    "        tp_1, tn_1, fp_1, fn_1 = get_instance_counts(y_true, y_pred, pos_label=1)\n",
    "        fpr_0 = fp_0 / (fp_0 + tn_0)\n",
    "        fpr_1 = fp_1 / (fp_1 + tn_1)\n",
    "        return (fpr_0 + fpr_1) / 2\n",
    "\n",
    "\n",
    "def positive_predicted_value(\n",
    "    y_true: List[int], y_pred: List[int], average: str = \"binary\", pos_label: int = 1\n",
    ") -> float:\n",
    "    # implemented as tp/(tp+fp)\n",
    "    # tp, tn, fp, fn = get_instance_counts(y_true, y_pred)\n",
    "    # return tp / (tp + fp)\n",
    "    return precision_score(\n",
    "        y_true=y_true, y_pred=y_pred, average=average, pos_label=pos_label\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dialect in [0, 1]:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"AAE\" if dialect == 1 else \"Non AAE\")\n",
    "    print(\"-\" * 80)\n",
    "    for label in SBIC_CATEGORICAL_COLUMNS:\n",
    "        print(\"=\" * 40)\n",
    "        print(label)\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        table_0_headers = [\n",
    "            \"Accuracy (0)\",\n",
    "            \"Precision (0)\",\n",
    "            \"Recall (0)\",\n",
    "            \"F1 (0)\",\n",
    "            \"TPR (0)\",\n",
    "            \"FPR (0)\",\n",
    "            \"PPV (0)\",\n",
    "        ]\n",
    "        table_0 = []\n",
    "        table_1_headers = [\n",
    "            \"Accuracy (1)\",\n",
    "            \"Precision (1)\",\n",
    "            \"Recall (1)\",\n",
    "            \"F1 (1)\",\n",
    "            \"TPR (1)\",\n",
    "            \"FPR (1)\",\n",
    "            \"PPV (1)\",\n",
    "        ]\n",
    "        table_1 = []\n",
    "        table_macro_headers = [\n",
    "            \"Accuracy\",\n",
    "            \"Precision (macro)\",\n",
    "            \"Recall (macro)\",\n",
    "            \"F1 (macro)\",\n",
    "            \"TPR (macro)\",\n",
    "            \"FPR (macro)\",\n",
    "            \"PPV (macro)\",\n",
    "        ]\n",
    "        table_macro = []\n",
    "\n",
    "        # mcnemar_pvalues = {}\n",
    "\n",
    "        all_approach_result_string = []\n",
    "        for approach_name in approaches:\n",
    "            approach_accuracy_0 = []\n",
    "            approach_precision_0 = []\n",
    "            approach_recall_0 = []\n",
    "            approach_f1_0 = []\n",
    "            approach_tpr_0 = []\n",
    "            approach_fpr_0 = []\n",
    "            approach_ppv_0 = []\n",
    "            approach_accuracy_1 = []\n",
    "            approach_precision_1 = []\n",
    "            approach_recall_1 = []\n",
    "            approach_f1_1 = []\n",
    "            approach_tpr_1 = []\n",
    "            approach_fpr_1 = []\n",
    "            approach_ppv_1 = []\n",
    "            approach_accuracy_macro = []\n",
    "            approach_precision_macro = []\n",
    "            approach_recall_macro = []\n",
    "            approach_f1_macro = []\n",
    "            approach_tpr_macro = []\n",
    "            approach_fpr_macro = []\n",
    "            approach_ppv_macro = []\n",
    "            for seed in SEEDS:\n",
    "                y_true = sbic_test[sbic_test[\"aae_dialect\"] == dialect][label]\n",
    "\n",
    "                if approach_name == \"majority\" or approach_name == \"random\":\n",
    "                    prediction = predictions_df[\n",
    "                        predictions_df[\"aae_dialect\"] == dialect\n",
    "                    ][f\"prediction_{label}_{approach_name}\"]\n",
    "                else:\n",
    "                    prediction = predictions_df[\n",
    "                        predictions_df[\"aae_dialect\"] == dialect\n",
    "                    ][f\"prediction_{label}_{approach_name}-seed{seed}\"]\n",
    "\n",
    "                # Accuracy\n",
    "                test_accuracy_0 = accuracy_score(y_true=y_true, y_pred=prediction)\n",
    "                # Precision, recall, f1\n",
    "                test_precision_0, test_recall_0, test_f1_0, support_0 = (\n",
    "                    precision_recall_fscore_support(\n",
    "                        y_true=y_true,\n",
    "                        y_pred=prediction,\n",
    "                        pos_label=0,\n",
    "                        average=\"binary\",\n",
    "                    )\n",
    "                )\n",
    "                # TPR\n",
    "                tpr_0 = true_positive_rate(\n",
    "                    y_true=y_true, y_pred=prediction, pos_label=0\n",
    "                )\n",
    "                # FPR\n",
    "                fpr_0 = false_positive_rate(\n",
    "                    y_true=y_true, y_pred=prediction, pos_label=0\n",
    "                )\n",
    "                # PPV\n",
    "                ppv_0 = positive_predicted_value(\n",
    "                    y_true=y_true, y_pred=prediction, pos_label=0\n",
    "                )\n",
    "\n",
    "                approach_accuracy_0.append(test_accuracy_0)\n",
    "                approach_precision_0.append(test_precision_0)\n",
    "                approach_recall_0.append(test_recall_0)\n",
    "                approach_f1_0.append(test_f1_0)\n",
    "                approach_tpr_0.append(tpr_0)\n",
    "                approach_fpr_0.append(fpr_0)\n",
    "                approach_ppv_0.append(ppv_0)\n",
    "                table_0.append(\n",
    "                    [\n",
    "                        f\"{approach_name}-seed{seed}\".capitalize(),\n",
    "                        np.round(test_accuracy_0, decimals=3),\n",
    "                        np.round(test_precision_0, decimals=3),\n",
    "                        np.round(test_recall_0, decimals=3),\n",
    "                        np.round(test_f1_0, decimals=3),\n",
    "                        np.round(tpr_0, decimals=3),\n",
    "                        np.round(fpr_0, decimals=3),\n",
    "                        np.round(ppv_0, decimals=3),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                test_accuracy_1 = accuracy_score(y_true=y_true, y_pred=prediction)\n",
    "                test_precision_1, test_recall_1, test_f1_1, support_1 = (\n",
    "                    precision_recall_fscore_support(\n",
    "                        y_true=y_true,\n",
    "                        y_pred=prediction,\n",
    "                        pos_label=1,\n",
    "                        average=\"binary\",\n",
    "                    )\n",
    "                )\n",
    "                # TPR\n",
    "                tpr_1 = true_positive_rate(\n",
    "                    y_true=y_true, y_pred=prediction, pos_label=1\n",
    "                )\n",
    "                # FPR\n",
    "                fpr_1 = false_positive_rate(\n",
    "                    y_true=y_true, y_pred=prediction, pos_label=1\n",
    "                )\n",
    "                # PPV\n",
    "                ppv_1 = positive_predicted_value(\n",
    "                    y_true=y_true, y_pred=prediction, pos_label=1\n",
    "                )\n",
    "                approach_accuracy_1.append(test_accuracy_1)\n",
    "                approach_precision_1.append(test_precision_1)\n",
    "                approach_recall_1.append(test_recall_1)\n",
    "                approach_f1_1.append(test_f1_1)\n",
    "                approach_tpr_1.append(tpr_1)\n",
    "                approach_fpr_1.append(fpr_1)\n",
    "                approach_ppv_1.append(ppv_1)\n",
    "                table_1.append(\n",
    "                    [\n",
    "                        f\"{approach_name}-seed{seed}\".capitalize(),\n",
    "                        np.round(test_accuracy_1, decimals=3),\n",
    "                        np.round(test_precision_1, decimals=3),\n",
    "                        np.round(test_recall_1, decimals=3),\n",
    "                        np.round(test_f1_1, decimals=3),\n",
    "                        np.round(tpr_1, decimals=3),\n",
    "                        np.round(fpr_1, decimals=3),\n",
    "                        np.round(ppv_1, decimals=3),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                test_accuracy_macro = 0\n",
    "                (\n",
    "                    test_precision_macro,\n",
    "                    test_recall_macro,\n",
    "                    test_f1_macro,\n",
    "                    support_macro,\n",
    "                ) = precision_recall_fscore_support(\n",
    "                    y_true=y_true,\n",
    "                    y_pred=prediction,\n",
    "                    average=\"macro\",\n",
    "                )\n",
    "                # TPR\n",
    "                tpr_macro = true_positive_rate(\n",
    "                    y_true=y_true, y_pred=prediction, average=\"macro\"\n",
    "                )\n",
    "                # FPR\n",
    "                fpr_macro = false_positive_rate(\n",
    "                    y_true=y_true, y_pred=prediction, average=\"macro\"\n",
    "                )\n",
    "                # PPV\n",
    "                ppv_macro = positive_predicted_value(\n",
    "                    y_true=y_true, y_pred=prediction, average=\"macro\"\n",
    "                )\n",
    "                approach_accuracy_macro.append(test_accuracy_macro)\n",
    "                approach_precision_macro.append(test_precision_macro)\n",
    "                approach_recall_macro.append(test_recall_macro)\n",
    "                approach_f1_macro.append(test_f1_macro)\n",
    "                approach_tpr_macro.append(tpr_macro)\n",
    "                approach_fpr_macro.append(fpr_macro)\n",
    "                approach_ppv_macro.append(ppv_macro)\n",
    "                table_macro.append(\n",
    "                    [\n",
    "                        f\"{approach_name}-seed{seed}\".capitalize(),\n",
    "                        test_accuracy_macro,\n",
    "                        np.round(test_precision_macro, decimals=3),\n",
    "                        np.round(test_recall_macro, decimals=3),\n",
    "                        np.round(test_f1_macro, decimals=3),\n",
    "                        np.round(tpr_macro, decimals=3),\n",
    "                        np.round(fpr_macro, decimals=3),\n",
    "                        np.round(ppv_macro, decimals=3),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # McNemar significance test\n",
    "                comparison_approach = f\"deberta-v3-base-finetune-seed{seed}\"\n",
    "                prediction_comparison = predictions_df[\n",
    "                    predictions_df[\"aae_dialect\"] == dialect\n",
    "                ][f\"prediction_{label}_{comparison_approach}\"]\n",
    "\n",
    "            table_0.append(\n",
    "                [\n",
    "                    f\"{approach_name}-average\".capitalize(),\n",
    "                    np.round(np.mean(approach_accuracy_0), decimals=3),\n",
    "                    np.round(np.mean(approach_precision_0), decimals=3),\n",
    "                    np.round(np.mean(approach_recall_0), decimals=3),\n",
    "                    np.round(np.mean(approach_f1_0), decimals=3),\n",
    "                    np.round(np.mean(approach_tpr_0), decimals=3),\n",
    "                    np.round(np.mean(approach_fpr_0), decimals=3),\n",
    "                    np.round(np.mean(approach_ppv_0), decimals=3),\n",
    "                ]\n",
    "            )\n",
    "            table_1.append(\n",
    "                [\n",
    "                    f\"{approach_name}-average\".capitalize(),\n",
    "                    np.round(np.mean(approach_accuracy_1), decimals=3),\n",
    "                    np.round(np.mean(approach_precision_1), decimals=3),\n",
    "                    np.round(np.mean(approach_recall_1), decimals=3),\n",
    "                    np.round(np.mean(approach_f1_1), decimals=3),\n",
    "                    np.round(np.mean(approach_tpr_1), decimals=3),\n",
    "                    np.round(np.mean(approach_fpr_1), decimals=3),\n",
    "                    np.round(np.mean(approach_ppv_1), decimals=3),\n",
    "                ]\n",
    "            )\n",
    "            table_macro.append(\n",
    "                [\n",
    "                    f\"{approach_name}-average\".capitalize(),\n",
    "                    np.round(np.mean(approach_accuracy_macro), decimals=3),\n",
    "                    np.round(np.mean(approach_precision_macro), decimals=3),\n",
    "                    np.round(np.mean(approach_recall_macro), decimals=3),\n",
    "                    np.round(np.mean(approach_f1_macro), decimals=3),\n",
    "                    np.round(np.mean(approach_tpr_macro), decimals=3),\n",
    "                    np.round(np.mean(approach_fpr_macro), decimals=3),\n",
    "                    np.round(np.mean(approach_ppv_macro), decimals=3),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        all_approach_result_string.append(\"Full results tables\")\n",
    "        all_approach_result_string.append(label)\n",
    "        if dialect == 0:\n",
    "            all_approach_result_string.append(\"no AAE\")\n",
    "        elif dialect == 1:\n",
    "            all_approach_result_string.append(\"AAE\")\n",
    "        for i in range(len(table_macro)):\n",
    "            if \"average\" in table_0[i][0] and \"Roberta\" in table_0[i][0]:\n",
    "                if dialect == 0:\n",
    "                    # print(\"no AAE\")\n",
    "                    all_approach_result_string.append(\n",
    "                        f\"{table_0[i][0]}: \"  # Approach naem\n",
    "                        f\"{table_0[i][1]},,,\"  # Accuracy\n",
    "                        f\"{table_1[i][2]},,{table_0[i][2]},,{table_macro[i][2]},,,\"  # Precision\n",
    "                        f\"{table_1[i][3]},,{table_0[i][3]},,{table_macro[i][3]},,,\"  # Recall\n",
    "                        f\"{table_1[i][4]},,{table_0[i][4]},,{table_macro[i][4]},,,\"  # F1\n",
    "                        f\"{table_1[i][5]},,,,\"  # TPR\n",
    "                        f\"{table_1[i][6]},,,,\"  # FPR\n",
    "                        f\"{table_1[i][7]},,,,\"  # PPV\n",
    "                    )\n",
    "                elif dialect == 1:\n",
    "                    # print(\"AAE\")\n",
    "                    all_approach_result_string.append(\n",
    "                        f\"{table_0[i][0]}: ,\"  # Approach naem\n",
    "                        f\"{table_0[i][1]},,,\"  # Accuracy\n",
    "                        f\"{table_1[i][2]},,{table_0[i][2]},,{table_macro[i][2]},,,\"  # Precision\n",
    "                        f\"{table_1[i][3]},,{table_0[i][3]},,{table_macro[i][3]},,,\"  # Recall\n",
    "                        f\"{table_1[i][4]},,{table_0[i][4]},,{table_macro[i][4]},,,\"  # F1\n",
    "                        f\"{table_1[i][5]},,,,\"  # TPR\n",
    "                        f\"{table_1[i][6]},,,,\"  # FPR\n",
    "                        f\"{table_1[i][7]},,,,\"  # PPV\n",
    "                    )\n",
    "\n",
    "        [print(i) for i in all_approach_result_string]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $t$-test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro, ttest_rel, ttest_1samp, wilcoxon, ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: if performance of A is significantly higher than performance of B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approach_to_test = \"\"\n",
    "alpha = 0.05\n",
    "n_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbic_test_splits = np.array_split(predictions_df, n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significance tests for selected approaches (with seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings to compare approach to respective ablations\n",
    "# approaches = [\"deberta-v3-base-joint-mtl-no-aae\", \"deberta-v3-base-joint-mtl\"]\n",
    "# baselines = [\"deberta-v3-base-two-task-mtl\", \"deberta-v3-base-finetune\"]\n",
    "# ttest_function = ttest_rel\n",
    "\n",
    "# Settings to compare AAE approaches to non-AAE variants\n",
    "approaches = [\"deberta-v3-base-two-task-mtl\", \"deberta-v3-base-joint-mtl\"]\n",
    "baselines = [\"deberta-v3-base-finetune\", \"deberta-v3-base-joint-mtl-no-aae\"]\n",
    "ttest_function = ttest_rel\n",
    "\n",
    "approach_pairs = [(x, y) for x in approaches for y in baselines]\n",
    "\n",
    "results = {}\n",
    "for dialect in [\"aae\", \"no-aae\", \"overall\"]:\n",
    "    results[dialect] = {}\n",
    "    if dialect == \"aae\":\n",
    "        dialect_value = 1\n",
    "    elif dialect == \"no-aae\":\n",
    "        dialect_value = 0\n",
    "    else:\n",
    "        dialect_value = -1\n",
    "\n",
    "    for label in SBIC_CATEGORICAL_COLUMNS:\n",
    "        print(\"=\" * 30)\n",
    "        print(dialect, label)\n",
    "        print(\"=\" * 30)\n",
    "        results[dialect][label] = {}\n",
    "\n",
    "        for score_index in [0, 1, 2]:\n",
    "            if score_index == 0:\n",
    "                print(\"\\n\\n\\n\")\n",
    "                print(\"Precision\")\n",
    "            elif score_index == 1:\n",
    "                print(\"\\n\\n\\n\")\n",
    "                print(\"Recall\")\n",
    "            elif score_index == 2:\n",
    "                print(\"\\n\\n\\n\")\n",
    "                print(\"F1 score\")\n",
    "\n",
    "            results[dialect][label][score_index] = {}\n",
    "\n",
    "            for pos_label in [0, 1, -1]:\n",
    "                if pos_label == 0:\n",
    "                    print(\"Negative label\")\n",
    "                elif pos_label == 1:\n",
    "                    print(\"Positive label\")\n",
    "                elif pos_label == -1:\n",
    "                    print(\"Macro averaged\")\n",
    "\n",
    "                results[dialect][label][score_index][pos_label] = {}\n",
    "\n",
    "                # Test for significance\n",
    "                for approach_A, approach_B in approach_pairs:\n",
    "                    approach_prediction_seeds = {}\n",
    "                    for approach in [approach_A, approach_B]:\n",
    "                        approach_prediction_seeds[approach] = {}\n",
    "                        if dialect_value == -1:\n",
    "                            if pos_label == -1:\n",
    "                                seed_scores = [\n",
    "                                    precision_recall_fscore_support(\n",
    "                                        y_pred=predictions_df[\n",
    "                                            f\"prediction_{label}_{approach}-seed{seed}\"\n",
    "                                        ],\n",
    "                                        y_true=predictions_df[label],\n",
    "                                        average=\"macro\",\n",
    "                                    )[score_index]\n",
    "                                    for seed in SEEDS\n",
    "                                ]\n",
    "                            else:\n",
    "                                seed_scores = [\n",
    "                                    precision_recall_fscore_support(\n",
    "                                        y_pred=predictions_df[\n",
    "                                            f\"prediction_{label}_{approach}-seed{seed}\"\n",
    "                                        ],\n",
    "                                        y_true=predictions_df[label],\n",
    "                                        average=\"binary\",\n",
    "                                        pos_label=pos_label,\n",
    "                                    )[score_index]\n",
    "                                    for seed in SEEDS\n",
    "                                ]\n",
    "                        else:\n",
    "                            if pos_label == -1:\n",
    "                                seed_scores = [\n",
    "                                    precision_recall_fscore_support(\n",
    "                                        y_pred=predictions_df[\n",
    "                                            predictions_df[\"aae_dialect\"]\n",
    "                                            == dialect_value\n",
    "                                        ][f\"prediction_{label}_{approach}-seed{seed}\"],\n",
    "                                        y_true=predictions_df[\n",
    "                                            predictions_df[\"aae_dialect\"]\n",
    "                                            == dialect_value\n",
    "                                        ][label],\n",
    "                                        average=\"macro\",\n",
    "                                    )[score_index]\n",
    "                                    for seed in SEEDS\n",
    "                                ]\n",
    "                            else:\n",
    "                                seed_scores = [\n",
    "                                    precision_recall_fscore_support(\n",
    "                                        y_pred=predictions_df[\n",
    "                                            predictions_df[\"aae_dialect\"]\n",
    "                                            == dialect_value\n",
    "                                        ][f\"prediction_{label}_{approach}-seed{seed}\"],\n",
    "                                        y_true=predictions_df[\n",
    "                                            predictions_df[\"aae_dialect\"]\n",
    "                                            == dialect_value\n",
    "                                        ][label],\n",
    "                                        average=\"binary\",\n",
    "                                        pos_label=pos_label,\n",
    "                                    )[score_index]\n",
    "                                    for seed in SEEDS\n",
    "                                ]\n",
    "                        approach_prediction_seeds[approach][label] = seed_scores\n",
    "\n",
    "                    scores_A = approach_prediction_seeds[approach_A][label]\n",
    "                    scores_B = approach_prediction_seeds[approach_B][label]\n",
    "\n",
    "                    if not np.mean(scores_A) > np.mean(scores_B):\n",
    "                        # print(\n",
    "                        #     f\"Skipped {approach_A} vs. {approach_B}; scores {np.mean(scores_A)} and {np.mean(scores_B)}\"\n",
    "                        # )\n",
    "                        continue\n",
    "                    # =======================================\n",
    "                    # Test for significance\n",
    "                    # Passing the scores of the baseline first, to ensure testing the correct hypothesis\n",
    "                    t_results = ttest_function(scores_B, scores_A)\n",
    "                    p_value = t_results[1] / 2\n",
    "\n",
    "                    if p_value <= alpha:\n",
    "                        print(\n",
    "                            f\"{approach_A} is significantly better than {approach_B} with p-value {p_value:.4f} (t-test).\"\n",
    "                        )\n",
    "                        results[dialect][label][score_index][pos_label][\n",
    "                            f\"{approach_A}___{approach_B}\"\n",
    "                        ] = f\"Significant with {p_value:.4f} (t-test)\"\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"{approach_A} is NOT significantly better than {approach_B} with p-value {p_value:.4f} (t-test).\"\n",
    "                        )\n",
    "                        results[dialect][label][score_index][pos_label][\n",
    "                            f\"{approach_A}___{approach_B}\"\n",
    "                        ] = f\"Not significant with {p_value:.4f} (t-test)\"\n",
    "                print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significance tests for selected approaches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings to compare approach to respective ablations\n",
    "# approaches = [\"deberta-v3-base-joint-mtl-no-aae\", \"deberta-v3-base-joint-mtl\"]\n",
    "# baselines = [\"deberta-v3-base-two-task-mtl\", \"deberta-v3-base-finetuned\"]\n",
    "# ttest_function = ttest_rel\n",
    "\n",
    "# Settings to compare AAE approaches to non-AAE variants\n",
    "approaches = [\"deberta-v3-base-two-task-mtl\", \"deberta-v3-base-joint-mtl\"]\n",
    "baselines = [\"deberta-v3-base-finetuned\", \"deberta-v3-base-joint-mtl-no-aae\"]\n",
    "ttest_function = ttest_rel\n",
    "\n",
    "approach_pairs = [(x, y) for x in approaches for y in baselines]\n",
    "\n",
    "results = {}\n",
    "for dialect in [\"aae\", \"no-aae\", \"overall\"]:\n",
    "    results[dialect] = {}\n",
    "    if dialect == \"aae\":\n",
    "        dialect_value = 1\n",
    "    elif dialect == \"no-aae\":\n",
    "        dialect_value = 0\n",
    "    else:\n",
    "        dialect_value = -1\n",
    "\n",
    "    for label in SBIC_CATEGORICAL_COLUMNS:\n",
    "        print(\"=\" * 30)\n",
    "        print(dialect, label)\n",
    "        print(\"=\" * 30)\n",
    "        results[dialect][label] = {}\n",
    "\n",
    "        for score_index in [0, 1, 2]:\n",
    "            if score_index == 0:\n",
    "                print(\"\\n\\n\\n\")\n",
    "                print(\"Precision\")\n",
    "            elif score_index == 1:\n",
    "                print(\"\\n\\n\\n\")\n",
    "                print(\"Recall\")\n",
    "            elif score_index == 2:\n",
    "                print(\"\\n\\n\\n\")\n",
    "                print(\"F1 score\")\n",
    "\n",
    "            results[dialect][label][score_index] = {}\n",
    "\n",
    "            for pos_label in [0, 1, -1]:\n",
    "                if pos_label == 0:\n",
    "                    print(\"Negative label\")\n",
    "                elif pos_label == 1:\n",
    "                    print(\"Positive label\")\n",
    "                elif pos_label == -1:\n",
    "                    print(\"Macro averaged\")\n",
    "\n",
    "                results[dialect][label][score_index][pos_label] = {}\n",
    "\n",
    "                # Test for significance\n",
    "                for approach_A, approach_B in approach_pairs:\n",
    "                    approach_prediction_splits = {}\n",
    "                    for approach in [approach_A, approach_B]:\n",
    "                        approach_prediction_splits[approach] = {}\n",
    "                        if dialect_value == -1:\n",
    "                            if pos_label == -1:\n",
    "                                split_scores = [\n",
    "                                    precision_recall_fscore_support(\n",
    "                                        y_pred=split[f\"prediction_{label}_{approach}\"],\n",
    "                                        y_true=split[label],\n",
    "                                        average=\"macro\",\n",
    "                                    )[score_index]\n",
    "                                    for split in sbic_test_splits\n",
    "                                ]\n",
    "                            else:\n",
    "                                split_scores = [\n",
    "                                    precision_recall_fscore_support(\n",
    "                                        y_pred=split[f\"prediction_{label}_{approach}\"],\n",
    "                                        y_true=split[label],\n",
    "                                        average=\"binary\",\n",
    "                                        pos_label=pos_label,\n",
    "                                    )[score_index]\n",
    "                                    for split in sbic_test_splits\n",
    "                                ]\n",
    "                        else:\n",
    "                            if pos_label == -1:\n",
    "                                split_scores = [\n",
    "                                    precision_recall_fscore_support(\n",
    "                                        y_pred=split[\n",
    "                                            split[\"aae_dialect\"] == dialect_value\n",
    "                                        ][f\"prediction_{label}_{approach}\"],\n",
    "                                        y_true=split[\n",
    "                                            split[\"aae_dialect\"] == dialect_value\n",
    "                                        ][label],\n",
    "                                        average=\"macro\",\n",
    "                                    )[score_index]\n",
    "                                    for split in sbic_test_splits\n",
    "                                ]\n",
    "                            else:\n",
    "                                split_scores = [\n",
    "                                    precision_recall_fscore_support(\n",
    "                                        y_pred=split[\n",
    "                                            split[\"aae_dialect\"] == dialect_value\n",
    "                                        ][f\"prediction_{label}_{approach}\"],\n",
    "                                        y_true=split[\n",
    "                                            split[\"aae_dialect\"] == dialect_value\n",
    "                                        ][label],\n",
    "                                        average=\"binary\",\n",
    "                                        pos_label=pos_label,\n",
    "                                    )[score_index]\n",
    "                                    for split in sbic_test_splits\n",
    "                                ]\n",
    "                        approach_prediction_splits[approach][label] = split_scores\n",
    "\n",
    "                    scores_A = approach_prediction_splits[approach_A][label]\n",
    "                    scores_B = approach_prediction_splits[approach_B][label]\n",
    "\n",
    "                    if not np.mean(scores_A) > np.mean(scores_B):\n",
    "                        continue\n",
    "                    # =======================================\n",
    "                    # Check for nomality\n",
    "                    score_differences = [a - b for a, b in zip(scores_A, scores_B)]\n",
    "\n",
    "                    # If normality > alpha, null hypothesis that test is normally distributed can be rejected\n",
    "                    normality = shapiro(score_differences)[1]\n",
    "\n",
    "                    # =======================================\n",
    "                    # Test for significance, with test depending on normality\n",
    "                    if normality > alpha:\n",
    "                        # Passing the scores of the baseline first, to ensure testing the correct hypothesis\n",
    "                        t_results = ttest_function(scores_B, scores_A)\n",
    "                        p_value = t_results[1] / 2\n",
    "\n",
    "                        if p_value <= alpha:\n",
    "                            print(\n",
    "                                f\"{approach_A} is significantly better than {approach_B} with p-value {p_value:.4f} (t-test).\"\n",
    "                            )\n",
    "                            results[dialect][label][score_index][pos_label][\n",
    "                                f\"{approach_A}___{approach_B}\"\n",
    "                            ] = f\"Significant with {p_value:.4f} (t-test)\"\n",
    "                        else:\n",
    "                            print(\n",
    "                                f\"{approach_A} is NOT significantly better than {approach_B} with p-value {p_value:.4f} (t-test).\"\n",
    "                            )\n",
    "                            results[dialect][label][score_index][pos_label][\n",
    "                                f\"{approach_A}___{approach_B}\"\n",
    "                            ] = f\"Not significant with {p_value:.4f} (t-test)\"\n",
    "                    else:\n",
    "                        # print(\"Normality test not significant. Ommitting test for now.\")\n",
    "                        results[dialect][label][score_index][pos_label][\n",
    "                            f\"{approach_A}___{approach_B}\"\n",
    "                        ] = \"No normal distribution\"\n",
    "                        # We can use the wilcoxon-signed rank test when normality is not given, as it is\n",
    "                        # non-parametric and thus does not make assumptions about the distribution\n",
    "                        w_results = wilcoxon(scores_B, scores_A)\n",
    "                        p_value = w_results[1]\n",
    "\n",
    "                        if p_value <= alpha:\n",
    "                            print(\n",
    "                                f\"{approach_A} is significantly better than {approach_B} with p-value {p_value:.4f} (wilcoxon).\"\n",
    "                            )\n",
    "                            results[dialect][label][score_index][pos_label][\n",
    "                                f\"{approach_A}___{approach_B}\"\n",
    "                            ] = f\"Significant with {p_value:.4f} (wilcoxon)\"\n",
    "                        else:\n",
    "                            print(\n",
    "                                f\"{approach_A} is NOT significantly better than {approach_B} with p-value {p_value:.4f} (wilcoxon).\"\n",
    "                            )\n",
    "                            results[dialect][label][score_index][pos_label][\n",
    "                                f\"{approach_A}___{approach_B}\"\n",
    "                            ] = f\"Not significant with {p_value:.4f} (wilcoxon)\"\n",
    "                print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significance test for fairness metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings to compare approach to respective ablations\n",
    "# approaches = [\"deberta-v3-base-joint-mtl-no-aae\", \"deberta-v3-base-joint-mtl\"]\n",
    "# baselines = [\"deberta-v3-base-two-task-mtl\", \"deberta-v3-base-finetune\"]\n",
    "# ttest_function = ttest_rel\n",
    "\n",
    "# Settings to compare AAE approaches to non-AAE variants\n",
    "approaches = [\"deberta-v3-base-two-task-mtl\", \"deberta-v3-base-joint-mtl\"]\n",
    "baselines = [\"deberta-v3-base-finetune\", \"deberta-v3-base-joint-mtl-no-aae\"]\n",
    "ttest_function = ttest_rel\n",
    "\n",
    "approach_pairs = [(x, y) for x in approaches for y in baselines]\n",
    "\n",
    "results = {}\n",
    "for dialect in [\"aae\", \"no-aae\", \"overall\"]:\n",
    "    results[dialect] = {}\n",
    "    if dialect == \"aae\":\n",
    "        dialect_value = 1\n",
    "    elif dialect == \"no-aae\":\n",
    "        dialect_value = 0\n",
    "    else:\n",
    "        dialect_value = -1\n",
    "\n",
    "    for label in SBIC_CATEGORICAL_COLUMNS:\n",
    "        print(\"=\" * 30)\n",
    "        print(dialect, label)\n",
    "        print(\"=\" * 30)\n",
    "        results[dialect][label] = {}\n",
    "\n",
    "        for score_index in [0, 1, 2]:\n",
    "            if score_index == 0:\n",
    "                print(\"\\n\\n\\n\")\n",
    "                print(\"True Positive Rate\")\n",
    "                metric_function = true_positive_rate\n",
    "            elif score_index == 1:\n",
    "                print(\"\\n\\n\\n\")\n",
    "                print(\"False Positive Rate\")\n",
    "                metric_function = false_positive_rate\n",
    "            elif score_index == 2:\n",
    "                print(\"\\n\\n\\n\")\n",
    "                print(\"Positive Predictive Value\")\n",
    "                metric_function = positive_predicted_value\n",
    "\n",
    "            results[dialect][label][score_index] = {}\n",
    "\n",
    "            for pos_label in [0, 1, -1]:\n",
    "                if pos_label == 0:\n",
    "                    print(\"Negative label\")\n",
    "                elif pos_label == 1:\n",
    "                    print(\"Positive label\")\n",
    "                elif pos_label == -1:\n",
    "                    print(\"Macro averaged\")\n",
    "\n",
    "                results[dialect][label][score_index][pos_label] = {}\n",
    "\n",
    "                # Test for significance\n",
    "                for approach_A, approach_B in approach_pairs:\n",
    "                    approach_prediction_splits = {}\n",
    "                    for approach in [approach_A, approach_B]:\n",
    "                        approach_prediction_splits[approach] = {}\n",
    "                        if dialect_value == -1:\n",
    "                            if pos_label == -1:\n",
    "                                seed_scores = [\n",
    "                                    metric_function(\n",
    "                                        y_true=predictions_df[label],\n",
    "                                        y_pred=predictions_df[\n",
    "                                            f\"prediction_{label}_{approach}-seed{seed}\"\n",
    "                                        ],\n",
    "                                        average=\"macro\",\n",
    "                                    )\n",
    "                                    for seed in SEEDS\n",
    "                                ]\n",
    "                            else:\n",
    "                                seed_scores = [\n",
    "                                    metric_function(\n",
    "                                        y_true=predictions_df[label],\n",
    "                                        y_pred=predictions_df[\n",
    "                                            f\"prediction_{label}_{approach}-seed{seed}\"\n",
    "                                        ],\n",
    "                                        pos_label=pos_label,\n",
    "                                    )\n",
    "                                    for seed in SEEDS\n",
    "                                ]\n",
    "                        else:\n",
    "                            if pos_label == -1:\n",
    "                                seed_scores = [\n",
    "                                    metric_function(\n",
    "                                        y_true=predictions_df[\n",
    "                                            predictions_df[\"aae_dialect\"]\n",
    "                                            == dialect_value\n",
    "                                        ][label],\n",
    "                                        y_pred=predictions_df[\n",
    "                                            predictions_df[\"aae_dialect\"]\n",
    "                                            == dialect_value\n",
    "                                        ][f\"prediction_{label}_{approach}-seed{seed}\"],\n",
    "                                        average=\"macro\",\n",
    "                                    )\n",
    "                                    for seed in SEEDS\n",
    "                                ]\n",
    "                            else:\n",
    "                                seed_scores = [\n",
    "                                    metric_function(\n",
    "                                        y_true=predictions_df[\n",
    "                                            predictions_df[\"aae_dialect\"]\n",
    "                                            == dialect_value\n",
    "                                        ][label],\n",
    "                                        y_pred=predictions_df[\n",
    "                                            predictions_df[\"aae_dialect\"]\n",
    "                                            == dialect_value\n",
    "                                        ][f\"prediction_{label}_{approach}-seed{seed}\"],\n",
    "                                        pos_label=pos_label,\n",
    "                                    )\n",
    "                                    for seed in SEEDS\n",
    "                                ]\n",
    "                        approach_prediction_splits[approach][label] = seed_scores\n",
    "\n",
    "                    scores_A = approach_prediction_splits[approach_A][label]\n",
    "                    scores_B = approach_prediction_splits[approach_B][label]\n",
    "\n",
    "                    # For FPR, we need to switch the test, as lower is better\n",
    "                    if score_index == 1:\n",
    "                        if not np.mean(scores_A) < np.mean(scores_B):\n",
    "                            continue\n",
    "                    else:\n",
    "                        if not np.mean(scores_A) > np.mean(scores_B):\n",
    "                            continue\n",
    "                    # =======================================\n",
    "                    # Check for nomality\n",
    "                    score_differences = [a - b for a, b in zip(scores_A, scores_B)]\n",
    "\n",
    "                    # If normality > alpha, null hypothesis that test is normally distributed can be rejected\n",
    "                    normality = shapiro(score_differences)[1]\n",
    "                    # Passing the scores of the baseline first, to ensure testing the correct hypothesis\n",
    "                    t_results = ttest_function(scores_B, scores_A)\n",
    "                    p_value = t_results[1] / 2\n",
    "\n",
    "                    if p_value <= alpha:\n",
    "                        print(\n",
    "                            f\"{approach_A} is significantly better than {approach_B} with p-value {p_value:.4f} (t-test).\"\n",
    "                        )\n",
    "                        results[dialect][label][score_index][pos_label][\n",
    "                            f\"{approach_A}___{approach_B}\"\n",
    "                        ] = f\"Significant with {p_value:.4f} (t-test)\"\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"{approach_A} is NOT significantly better than {approach_B} with p-value {p_value:.4f} (t-test).\"\n",
    "                        )\n",
    "                        results[dialect][label][score_index][pos_label][\n",
    "                            f\"{approach_A}___{approach_B}\"\n",
    "                        ] = f\"Not significant with {p_value:.4f} (t-test)\"\n",
    "                print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significance tests for no-code baselines\n",
    "\n",
    "(one-sample $t$-test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approaches = [\n",
    "    \"deberta-v3-base-joint-mtl-no-aae\",\n",
    "    \"deberta-v3-base-joint-mtl\",\n",
    "    \"deberta-v3-base-two-task-mtl\",\n",
    "    \"deberta-v3-base-finetune\",\n",
    "]\n",
    "no_code_baselines = {\n",
    "    \"GPT-2\": {\n",
    "        \"offensiveYN\": 0.788,\n",
    "        \"intentYN\": 0.786,\n",
    "        \"lewdYN\": 0.807,\n",
    "        \"groupYN\": 0.699,\n",
    "        \"ingroupYN\": 0.000,\n",
    "    },\n",
    "    \"Few-shot learning\": {\n",
    "        \"offensiveYN\": 0.822,\n",
    "        \"intentYN\": 0.798,\n",
    "        \"lewdYN\": 0.411,\n",
    "        \"groupYN\": 0.737,\n",
    "        \"ingroupYN\": 0.000,\n",
    "    },\n",
    "}\n",
    "approach_no_code_baseline_pairs = [\n",
    "    (a, b) for a in approaches for b in no_code_baselines.keys()\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for label in SBIC_CATEGORICAL_COLUMNS:\n",
    "    results[label] = {}\n",
    "\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"For label '{label}'\")\n",
    "    print(\"=\" * 30)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    approach_prediction_splits = {}\n",
    "    for approach in approaches:\n",
    "        seed_scores = [\n",
    "            precision_recall_fscore_support(\n",
    "                y_pred=predictions_df[f\"prediction_{label}_{approach}-seed{seed}\"],\n",
    "                y_true=predictions_df[label],\n",
    "                average=\"binary\",\n",
    "                pos_label=1,\n",
    "            )[2]\n",
    "            for seed in SEEDS\n",
    "        ]\n",
    "        approach_prediction_splits[approach] = seed_scores\n",
    "\n",
    "    # Test for significance\n",
    "    for approach_A, approach_B in approach_no_code_baseline_pairs:\n",
    "        scores_A = approach_prediction_splits[approach_A]\n",
    "        score_B = no_code_baselines[approach_B][label]\n",
    "\n",
    "        if (np.mean(scores_A) == 0.0) or (not np.mean(scores_A) > score_B):\n",
    "            results[label][\n",
    "                f\"{approach_A}___{approach_B}\"\n",
    "            ] = \"Not better than second approach\"\n",
    "            continue\n",
    "        # =======================================\n",
    "        # Check for nomality\n",
    "        score_differences = [a - score_B for a in scores_A]\n",
    "\n",
    "        # Since we have a large sample size, we can use the one-sampled t-test even with skewed and\n",
    "        # non-normally distributed data, as per https://stats.libretexts.org/Bookshelves/Applied_Statistics/Biological_Statistics_(McDonald)/04%3A_Tests_for_One_Measurement_Variable/4.01%3A_One-Sample_t-Test ; visited on 2024-02-01\n",
    "        # (see section \"Assumptions\")\n",
    "\n",
    "        # =======================================\n",
    "        # Test for significance, without test depending on normality\n",
    "        # two sided t-test, one sampled\n",
    "        t_results = ttest_1samp(scores_A, score_B)\n",
    "        # correct for one sided test, according to Hitchhiker's guide\n",
    "        p_value = t_results[1] / 2\n",
    "\n",
    "        if p_value <= alpha:\n",
    "            print(\n",
    "                f\"{approach_A} is significantly better than {approach_B} with p-value {p_value:.4f}.\"\n",
    "            )\n",
    "            results[label][\n",
    "                f\"{approach_A}___{approach_B}\"\n",
    "            ] = f\"Significant with {p_value:.4f}\"\n",
    "        else:\n",
    "            print(\n",
    "                f\"{approach_A} is NOT significantly better than {approach_B} with p-value {p_value:.4f}.\"\n",
    "            )\n",
    "            results[label][\n",
    "                f\"{approach_A}___{approach_B}\"\n",
    "            ] = f\"Not significant with {p_value:.4f}\"\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(results, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialect-bias-detection-XNYuKIKE-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
