{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all the code necessary to prepare the different datasets and sources. The different preprocessing steps are explained in more details below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import SBIC_OFFENSIVENESS_THRESHOLD, SBIC_SEXUAL_THRESHOLD, RANDOM_SEED\n",
    "from util import preprocess_twitter_texts\n",
    "from os import getcwd, path\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../common\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = Path(path.abspath(\"\")).parent\n",
    "DATA_DIR = path.join(parent_path, \"data\")\n",
    "INTERMEDIATE_DIR = path.join(parent_path, \"intermediate\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Bias Frames Corpus\n",
    "\n",
    "Also referred to as _SBIC_ throughout the codebase. The corpus can be retrieved from https://maartensap.com/social-bias-frames/ (in case the link is not reachable anymore, the data can also be retrieved from the internet archive: https://web.archive.org/web/20230523165906/https://maartensap.com/social-bias-frames/ ). We decided to use the \"aggregated by post\" data, as we don't need the single annotator annotations. This version is furthermore already deduplicated.\n",
    "\n",
    "The code expects the following files from the corpus in the following directories:\n",
    "\n",
    "- `SBIC.v2.agg.trn.csv` in `data/common/`\n",
    "- `SBIC.v2.agg.dev.csv` in `data/common/`\n",
    "- `SBIC.v2.agg.tst.csv` in `data/common/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SBIC data from file\n",
    "sbic_train = pd.read_csv(path.join(DATA_DIR, \"common\", \"SBIC.v2.agg.trn.csv\"))\n",
    "sbic_val = pd.read_csv(path.join(DATA_DIR, \"common\", \"SBIC.v2.agg.dev.csv\"))\n",
    "sbic_test = pd.read_csv(path.join(DATA_DIR, \"common\", \"SBIC.v2.agg.tst.csv\"))\n",
    "\n",
    "# Extract columns of interest and rename ID column\n",
    "columns_of_interest = [\"post\", \"sexYN\", \"offensiveYN\", \"intentYN\", \"Unnamed: 0\", \"dataSource\"]\n",
    "sbic_train = sbic_train[columns_of_interest].rename(columns={\"Unnamed: 0\": \"post_id\"})\n",
    "sbic_val = sbic_val[columns_of_interest].rename(columns={\"Unnamed: 0\": \"post_id\"})\n",
    "sbic_test = sbic_test[columns_of_interest].rename(columns={\"Unnamed: 0\": \"post_id\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_social_bias_labels(df):\n",
    "    \"\"\"Transform continuous values to binary labels.\n",
    "\n",
    "    Also create a new binary column \"label\" that combines the values of offensive and sexual to a\n",
    "    new \"biased\" column.\n",
    "\n",
    "    1: offensive or sexual (depending on the column)\n",
    "    0: neither\n",
    "\n",
    "    This function modifies the provided dataframe inplace, so no return value is given.\n",
    "    \"\"\"\n",
    "    df[\"lewdness_label\"] = np.where(df[\"sexYN\"] >= SBIC_SEXUAL_THRESHOLD, 1, 0)\n",
    "    df[\"offensiveness_label\"] = np.where(df[\"offensiveYN\"] >= SBIC_OFFENSIVENESS_THRESHOLD, 1, 0)\n",
    "    df[\"social_bias_label\"] = df.apply(\n",
    "        lambda x: 1 if (x[\"lewdness_label\"] + x[\"offensiveness_label\"]) > 0 else 0, axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "# Preprocess text columns\n",
    "sbic_train_prep = preprocess_twitter_texts(sbic_train, target_text_colum=\"post_preprocessed\")\n",
    "sbic_val_prep = preprocess_twitter_texts(sbic_val, target_text_colum=\"post_preprocessed\")\n",
    "sbic_test_prep = preprocess_twitter_texts(sbic_test, target_text_colum=\"post_preprocessed\")\n",
    "\n",
    "# Create binary labels for each data split\n",
    "infer_social_bias_labels(sbic_train_prep)\n",
    "infer_social_bias_labels(sbic_val_prep)\n",
    "infer_social_bias_labels(sbic_test_prep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns we are interested in for later tasks\n",
    "sbic_train_prep = sbic_train_prep[\n",
    "    [\"post_id\", \"post_preprocessed\", \"social_bias_label\", \"offensiveness_label\", \"lewdness_label\"]\n",
    "].copy()\n",
    "sbic_val_prep = sbic_val_prep[\n",
    "    [\"post_id\", \"post_preprocessed\", \"social_bias_label\", \"offensiveness_label\", \"lewdness_label\"]\n",
    "].copy()\n",
    "sbic_test_prep = sbic_test_prep[\n",
    "    [\"post_id\", \"post_preprocessed\", \"social_bias_label\", \"offensiveness_label\", \"lewdness_label\"]\n",
    "].copy()\n",
    "\n",
    "# Clean up data from either empty posts or hand-selected examples that don't meet our minimum\n",
    "# requirements\n",
    "sbic_train_prep.drop(\n",
    "    sbic_train_prep[sbic_train_prep[\"post_preprocessed\"].str.len() == 0].index, inplace=True\n",
    ")\n",
    "sbic_train_prep.drop(\n",
    "    sbic_train_prep[sbic_train_prep[\"post_id\"].isin([1, 35493, 35495])].index, inplace=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write final prepared data to file\n",
    "sbic_train_prep.to_csv(\n",
    "    path.join(INTERMEDIATE_DIR, \"common\", \"sbic-train-prep.csv\"), sep=\",\", index=False\n",
    ")\n",
    "sbic_val_prep.to_csv(\n",
    "    path.join(INTERMEDIATE_DIR, \"common\", \"sbic-val-prep.csv\"), sep=\",\", index=False\n",
    ")\n",
    "sbic_test_prep.to_csv(\n",
    "    path.join(INTERMEDIATE_DIR, \"common\", \"sbic-test-prep.csv\"), sep=\",\", index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TwitterAAE corpus\n",
    "\n",
    "The corpus can be retrieved from http://slanglab.cs.umass.edu/TwitterAAE/. It is later used to train a AAE dialect classifier, which is in turn used to annotate the SBIC data for containing AAE dialect.\n",
    "\n",
    "In order to create a balanced dataset, we randomly subsample the non-AAE Twitter posts to the size of the AAE posts data.\n",
    "\n",
    "The code expects the following files from the corpus in the following directories:\n",
    "\n",
    "- `twitteraae_all_aa` in `data/common/`\n",
    "- `twitteraae_all` in `data/common/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TwitterAAE data from file\n",
    "twitter_data_full = pd.read_csv(\n",
    "    path.join(DATA_DIR, \"common\", \"twitteraae_all\"),\n",
    "    sep=\"\\t\",\n",
    "    escapechar=\"\\\\\",\n",
    "    names=[\n",
    "        \"post_id\",\n",
    "        \"timestamp\",\n",
    "        \"user_id\",\n",
    "        \"location\",\n",
    "        \"census_blk_group\",\n",
    "        \"post\",\n",
    "        \"demograpic1_inference\",\n",
    "        \"demograpic2_inference\",\n",
    "        \"demograpic3_inference\",\n",
    "        \"demograpic4_inference\",\n",
    "    ],\n",
    ")\n",
    "twitter_data_aae = pd.read_csv(\n",
    "    path.join(DATA_DIR, \"common\", \"twitteraae_all_aa\"),\n",
    "    sep=\"\\t\",\n",
    "    escapechar=\"\\\\\",\n",
    "    names=[\n",
    "        \"post_id\",\n",
    "        \"timestamp\",\n",
    "        \"user_id\",\n",
    "        \"location\",\n",
    "        \"census_blk_group\",\n",
    "        \"post\",\n",
    "        \"demograpic1_inference\",\n",
    "        \"demograpic2_inference\",\n",
    "        \"demograpic3_inference\",\n",
    "        \"demograpic4_inference\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Since the full dataset is a superset of the AAE dataset, we need to filter AAE samples\n",
    "twitter_data_no_aae = twitter_data_full[\n",
    "    ~twitter_data_full.post_id.isin(twitter_data_aae.post_id)\n",
    "].copy()\n",
    "\n",
    "# Extract columns of interest\n",
    "columns_of_interest = [\"post_id\", \"post\"]\n",
    "twitter_data_aae = twitter_data_aae[columns_of_interest]\n",
    "twitter_data_no_aae = twitter_data_no_aae[columns_of_interest]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text columns\n",
    "twitter_data_aae_prep = preprocess_twitter_texts(twitter_data_aae)\n",
    "twitter_data_no_aae_prep = preprocess_twitter_texts(twitter_data_no_aae)\n",
    "\n",
    "# Remove empty samples that might have appeared after preprocessing\n",
    "twitter_data_aae_prep.dropna(inplace=True)\n",
    "twitter_data_no_aae_prep.dropna(inplace=True)\n",
    "twitter_data_aae_prep = twitter_data_aae_prep[twitter_data_aae_prep.post_preprocessed != \"\"]\n",
    "twitter_data_no_aae_prep = twitter_data_no_aae_prep[\n",
    "    twitter_data_no_aae_prep.post_preprocessed != \"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample AAE posts\n",
    "twitter_data_no_aae_prep_sampled = twitter_data_no_aae_prep.sample(\n",
    "    n=len(twitter_data_aae_prep), random_state=RANDOM_SEED, axis=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge AAE and non AAE posts into a single dataframe\n",
    "twitter_data_all_labels = twitter_data_aae_prep.merge(twitter_data_no_aae_prep_sampled, how=\"outer\")\n",
    "twitter_data_all_labels[\"aae_dialect_label\"] = twitter_data_all_labels.post_id.apply(\n",
    "    lambda x: 1 if x in twitter_data_aae.post_id.values else 0\n",
    ")\n",
    "\n",
    "# Only select the columns of interest\n",
    "twitter_data_all_labels = twitter_data_all_labels[\n",
    "    [\"post_id\", \"post_preprocessed\", \"aae_dialect_label\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stratified data splits for training and evaluation\n",
    "twitter_data_train, twitter_data_test = train_test_split(\n",
    "    twitter_data_all_labels,\n",
    "    train_size=0.8,\n",
    "    stratify=twitter_data_all_labels[\"aae_dialect_label\"],\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "twitter_data_train, twitter_data_val = train_test_split(\n",
    "    twitter_data_train,\n",
    "    train_size=0.8,\n",
    "    stratify=twitter_data_train[\"aae_dialect_label\"],\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write final prepared data to file\n",
    "twitter_data_train.to_csv(\n",
    "    path.join(INTERMEDIATE_DIR, \"common\", \"twitteraae-train-prep.csv\"), sep=\",\", index=False\n",
    ")\n",
    "twitter_data_val.to_csv(\n",
    "    path.join(INTERMEDIATE_DIR, \"common\", \"twitteraae-val-prep.csv\"), sep=\",\", index=False\n",
    ")\n",
    "twitter_data_test.to_csv(\n",
    "    path.join(INTERMEDIATE_DIR, \"common\", \"twitteraae-test-prep.csv\"), sep=\",\", index=False\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialect-bias-detection-XqPNrE44-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
