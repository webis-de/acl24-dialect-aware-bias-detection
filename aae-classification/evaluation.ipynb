{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TwitterAAE classification evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitteraae_test = pd.read_csv(\n",
    "    \"intermediate/twitter-aae/twitteraae-test-labeled-prep.csv\"\n",
    ")\n",
    "twitteraae_baseline = pd.read_csv(\"output/twitteraae-test_predictions-baseline.csv\")\n",
    "twitteraae_finetuned = pd.read_csv(\n",
    "    \"output/twitteraae-test_predictions-deberta-v3-base-aee-classifier.csv\"\n",
    ")\n",
    "twitteraae_finetuned_subsample = pd.read_csv(\n",
    "    \"output/twitteraae-test_predictions-deberta-v3-base-aee-classifier-interleaving.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority baseline\n",
    "majority_label = twitteraae_test[\"aae_dialect_label\"].value_counts(sort=True).index[0]\n",
    "majority_predictions = np.full(len(twitteraae_test), majority_label)\n",
    "\n",
    "# Pseudo-random baseline\n",
    "random_predictions = np.random.randint(2, size=len(twitteraae_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    \"baseline_majority\": majority_predictions,\n",
    "    \"baseline_random\": random_predictions,\n",
    "    \"baseline_twitteraae\": twitteraae_baseline[\"prediction_twitteraae_baseline\"],\n",
    "    \"approach_finetuned\": twitteraae_finetuned[\n",
    "        \"prediction_deberta-v3-base-aee-classifier\"\n",
    "    ],\n",
    "    \"approach_finetuned_subsample\": twitteraae_finetuned_subsample[\n",
    "        \"prediction_deberta-v3-base-aee-classifier-interleaving\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = twitteraae_test.copy()\n",
    "predictions_df[\"prediction_baseline_majority\"] = majority_predictions\n",
    "predictions_df[\"prediction_baseline_random\"] = random_predictions\n",
    "predictions_df[\"prediction_baseline_twitteraae\"] = twitteraae_baseline[\n",
    "    \"prediction_twitteraae_baseline\"\n",
    "]\n",
    "predictions_df[\"prediction_approach_finetuned\"] = twitteraae_finetuned[\n",
    "    \"prediction_deberta-v3-base-aee-classifier\"\n",
    "]\n",
    "predictions_df[\"prediction_approach_finetuned_subsample\"] = (\n",
    "    twitteraae_finetuned_subsample[\n",
    "        \"prediction_deberta-v3-base-aee-classifier-interleaving\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_aae_samples = predictions_df[predictions_df[\"aae_dialect_label\"] == 1]\n",
    "aae_subsample = predictions_df[predictions_df[\"aae_dialect_label\"] == 0].sample(\n",
    "    n=len(no_aae_samples), random_state=23\n",
    ")\n",
    "predictions_subsampled = pd.concat([no_aae_samples, aae_subsample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class distribution (test set)\")\n",
    "len_neg = len(twitteraae_test[twitteraae_test[\"aae_dialect_label\"] == 0])\n",
    "len_pos = len(twitteraae_test[twitteraae_test[\"aae_dialect_label\"] == 1])\n",
    "print(f\"Non-AAE:\\t{len_neg} ({len_neg / len(twitteraae_test):.2f})\")\n",
    "print(f\"AAE:\\t\\t{len_pos} ({len_pos / len(twitteraae_test):.2f})\")\n",
    "print(\"--> Positive class is minority class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class distribution (sampled test set)\")\n",
    "len_neg = len(predictions_subsampled[predictions_subsampled[\"aae_dialect_label\"] == 0])\n",
    "len_pos = len(predictions_subsampled[predictions_subsampled[\"aae_dialect_label\"] == 1])\n",
    "print(f\"Non-AAE:\\t{len_neg} ({len_neg / len(predictions_subsampled):.2f})\")\n",
    "print(f\"AAE:\\t\\t{len_pos} ({len_pos / len(predictions_subsampled):.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = twitteraae_test[\"aae_dialect_label\"]\n",
    "for approach_name, prediction in predictions.items():\n",
    "    print(\"=\" * 20)\n",
    "    print(approach_name)\n",
    "    print(\"True positive:\\t\", np.sum((y_true == 1) & (prediction == 1)))\n",
    "    print(\"False positive:\\t\", np.sum((y_true == 0) & (prediction == 1)))\n",
    "    print(\"True negative:\\t\", np.sum((y_true == 0) & (prediction == 0)))\n",
    "    print(\"False negative:\\t\", np.sum((y_true == 1) & (prediction == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for approach_name in predictions.keys():\n",
    "    print(\"=\" * 20)\n",
    "    print(approach_name)\n",
    "\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        predictions_df[\"aae_dialect_label\"],\n",
    "        predictions_df[f\"prediction_{approach_name}\"],\n",
    "        normalize=\"true\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampled test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = predictions_subsampled[\"aae_dialect_label\"]\n",
    "for approach_name in predictions.keys():\n",
    "    print(\"=\" * 20)\n",
    "    print(approach_name)\n",
    "    print(\n",
    "        \"True positive:\\t\",\n",
    "        np.sum(\n",
    "            (y_true == 1) & (predictions_subsampled[f\"prediction_{approach_name}\"] == 1)\n",
    "        ),\n",
    "    )\n",
    "    print(\n",
    "        \"False positive:\\t\",\n",
    "        np.sum(\n",
    "            (y_true == 0) & (predictions_subsampled[f\"prediction_{approach_name}\"] == 1)\n",
    "        ),\n",
    "    )\n",
    "    print(\n",
    "        \"True negative:\\t\",\n",
    "        np.sum(\n",
    "            (y_true == 0) & (predictions_subsampled[f\"prediction_{approach_name}\"] == 0)\n",
    "        ),\n",
    "    )\n",
    "    print(\n",
    "        \"False negative:\\t\",\n",
    "        np.sum(\n",
    "            (y_true == 1) & (predictions_subsampled[f\"prediction_{approach_name}\"] == 0)\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for approach_name in predictions.keys():\n",
    "    print(\"=\" * 20)\n",
    "    print(approach_name)\n",
    "\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        predictions_subsampled[\"aae_dialect_label\"],\n",
    "        predictions_subsampled[f\"prediction_{approach_name}\"],\n",
    "        normalize=\"true\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_0_headers = [\"Accuracy (0)\", \"Precision (0)\", \"Recall (0)\", \"F1 (0)\"]\n",
    "table_0 = []\n",
    "table_1_headers = [\"Accuracy (1)\", \"Precision (1)\", \"Recall (1)\", \"F1 (1)\"]\n",
    "table_1 = []\n",
    "table_macro_headers = [\"Accuracy\", \"Precision (macro)\", \"Recall (macro)\", \"F1 (macro)\"]\n",
    "table_macro = []\n",
    "\n",
    "\n",
    "for approach_name, prediction in predictions.items():\n",
    "    test_accuracy_0 = accuracy_score(\n",
    "        y_true=twitteraae_test[\"aae_dialect_label\"], y_pred=prediction\n",
    "    )\n",
    "    test_precision_0, test_recall_0, test_f1_0, support_0 = (\n",
    "        precision_recall_fscore_support(\n",
    "            y_true=twitteraae_test[\"aae_dialect_label\"],\n",
    "            y_pred=prediction,\n",
    "            pos_label=0,\n",
    "            average=\"binary\",\n",
    "        )\n",
    "    )\n",
    "    table_0.append(\n",
    "        [\n",
    "            approach_name,\n",
    "            np.round(test_accuracy_0, decimals=3),\n",
    "            np.round(test_precision_0, decimals=3),\n",
    "            np.round(test_recall_0, decimals=3),\n",
    "            np.round(test_f1_0, decimals=3),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_accuracy_1 = accuracy_score(\n",
    "        y_true=twitteraae_test[\"aae_dialect_label\"], y_pred=prediction\n",
    "    )\n",
    "    test_precision_1, test_recall_1, test_f1_1, support_1 = (\n",
    "        precision_recall_fscore_support(\n",
    "            y_true=twitteraae_test[\"aae_dialect_label\"],\n",
    "            y_pred=prediction,\n",
    "            pos_label=1,\n",
    "            average=\"binary\",\n",
    "        )\n",
    "    )\n",
    "    table_1.append(\n",
    "        [\n",
    "            approach_name,\n",
    "            np.round(test_accuracy_1, decimals=3),\n",
    "            np.round(test_precision_1, decimals=3),\n",
    "            np.round(test_recall_1, decimals=3),\n",
    "            np.round(test_f1_1, decimals=3),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_accuracy_macro = \"-\"\n",
    "    (\n",
    "        test_precision_macro,\n",
    "        test_recall_macro,\n",
    "        test_f1_macro,\n",
    "        support_macro,\n",
    "    ) = precision_recall_fscore_support(\n",
    "        y_true=twitteraae_test[\"aae_dialect_label\"],\n",
    "        y_pred=prediction,\n",
    "        average=\"macro\",\n",
    "    )\n",
    "    table_macro.append(\n",
    "        [\n",
    "            approach_name,\n",
    "            test_accuracy_macro,\n",
    "            np.round(test_precision_macro, decimals=3),\n",
    "            np.round(test_recall_macro, decimals=3),\n",
    "            np.round(test_f1_macro, decimals=3),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(\"=\" * 20)\n",
    "print(\"Negative class binary scores\")\n",
    "print(tabulate(table_0, headers=table_0_headers, tablefmt=\"rounded_grid\"))\n",
    "\n",
    "print(\"=\" * 20)\n",
    "print(\"Positive class binary scores\")\n",
    "print(tabulate(table_1, headers=table_1_headers, tablefmt=\"rounded_grid\"))\n",
    "\n",
    "print(\"=\" * 20)\n",
    "print(\"Macro averaged scores\")\n",
    "print(tabulate(table_macro, headers=table_macro_headers, tablefmt=\"rounded_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampled test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_0_headers = [\"Accuracy (0)\", \"Precision (0)\", \"Recall (0)\", \"F1 (0)\"]\n",
    "table_0 = []\n",
    "table_1_headers = [\"Accuracy (1)\", \"Precision (1)\", \"Recall (1)\", \"F1 (1)\"]\n",
    "table_1 = []\n",
    "table_macro_headers = [\"Accuracy\", \"Precision (macro)\", \"Recall (macro)\", \"F1 (macro)\"]\n",
    "table_macro = []\n",
    "\n",
    "\n",
    "for approach_name in predictions.keys():\n",
    "    prediction = predictions_subsampled[f\"prediction_{approach_name}\"]\n",
    "    y_true = predictions_subsampled[\"aae_dialect_label\"]\n",
    "\n",
    "    test_accuracy_0 = accuracy_score(y_true=y_true, y_pred=prediction)\n",
    "    test_precision_0, test_recall_0, test_f1_0, support_0 = (\n",
    "        precision_recall_fscore_support(\n",
    "            y_true=y_true,\n",
    "            y_pred=prediction,\n",
    "            pos_label=0,\n",
    "            average=\"binary\",\n",
    "        )\n",
    "    )\n",
    "    table_0.append(\n",
    "        [\n",
    "            approach_name,\n",
    "            np.round(test_accuracy_0, decimals=3),\n",
    "            np.round(test_precision_0, decimals=3),\n",
    "            np.round(test_recall_0, decimals=3),\n",
    "            np.round(test_f1_0, decimals=3),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_accuracy_1 = accuracy_score(y_true=y_true, y_pred=prediction)\n",
    "    test_precision_1, test_recall_1, test_f1_1, support_1 = (\n",
    "        precision_recall_fscore_support(\n",
    "            y_true=y_true,\n",
    "            y_pred=prediction,\n",
    "            pos_label=1,\n",
    "            average=\"binary\",\n",
    "        )\n",
    "    )\n",
    "    table_1.append(\n",
    "        [\n",
    "            approach_name,\n",
    "            np.round(test_accuracy_1, decimals=3),\n",
    "            np.round(test_precision_1, decimals=3),\n",
    "            np.round(test_recall_1, decimals=3),\n",
    "            np.round(test_f1_1, decimals=3),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_accuracy_macro = \"-\"\n",
    "    (\n",
    "        test_precision_macro,\n",
    "        test_recall_macro,\n",
    "        test_f1_macro,\n",
    "        support_macro,\n",
    "    ) = precision_recall_fscore_support(\n",
    "        y_true=y_true,\n",
    "        y_pred=prediction,\n",
    "        average=\"macro\",\n",
    "    )\n",
    "    table_macro.append(\n",
    "        [\n",
    "            approach_name,\n",
    "            test_accuracy_macro,\n",
    "            np.round(test_precision_macro, decimals=3),\n",
    "            np.round(test_recall_macro, decimals=3),\n",
    "            np.round(test_f1_macro, decimals=3),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(\"=\" * 20)\n",
    "print(\"Negative class binary scores\")\n",
    "print(tabulate(table_0, headers=table_0_headers, tablefmt=\"rounded_grid\"))\n",
    "\n",
    "print(\"=\" * 20)\n",
    "print(\"Positive class binary scores\")\n",
    "print(tabulate(table_1, headers=table_1_headers, tablefmt=\"rounded_grid\"))\n",
    "\n",
    "print(\"=\" * 20)\n",
    "print(\"Macro averaged scores\")\n",
    "print(tabulate(table_macro, headers=table_macro_headers, tablefmt=\"rounded_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $t$-test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro, ttest_ind, wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "n_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For full test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitteraae_test_splits = np.array_split(predictions_df, n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approaches = [\"approach_finetuned\", \"approach_finetuned_subsample\"]\n",
    "baselines = [\"baseline_twitteraae\"]\n",
    "\n",
    "for score_index in [0, 1, 2]:\n",
    "    if score_index == 0:\n",
    "        print(\"\\n\\n\\n\")\n",
    "        print(\"Precision\")\n",
    "    elif score_index == 1:\n",
    "        print(\"\\n\\n\\n\")\n",
    "        print(\"Recall\")\n",
    "    elif score_index == 2:\n",
    "        print(\"\\n\\n\\n\")\n",
    "        print(\"F1 score\")\n",
    "\n",
    "    for pos_label in [0, 1, -1]:\n",
    "        if pos_label == 0:\n",
    "            print(\"Negative label\")\n",
    "        elif pos_label == 1:\n",
    "            print(\"Positive label\")\n",
    "        elif pos_label == -1:\n",
    "            print(\"Macro averaged\")\n",
    "\n",
    "        approach_prediction_splits = {}\n",
    "        for approach in [*approaches, *baselines]:\n",
    "            if pos_label == -1:\n",
    "                split_scores = [\n",
    "                    precision_recall_fscore_support(\n",
    "                        y_pred=split[f\"prediction_{approach}\"],\n",
    "                        y_true=split[\"aae_dialect_label\"],\n",
    "                        average=\"macro\",\n",
    "                    )[score_index]\n",
    "                    for split in twitteraae_test_splits\n",
    "                ]\n",
    "            else:\n",
    "                split_scores = [\n",
    "                    precision_recall_fscore_support(\n",
    "                        y_pred=split[f\"prediction_{approach}\"],\n",
    "                        y_true=split[\"aae_dialect_label\"],\n",
    "                        average=\"binary\",\n",
    "                        pos_label=pos_label,\n",
    "                    )[score_index]\n",
    "                    for split in twitteraae_test_splits\n",
    "                ]\n",
    "            approach_prediction_splits[approach] = split_scores\n",
    "\n",
    "        approach_pairs = [(x, y) for x in approaches for y in baselines]\n",
    "        results = {}\n",
    "        # Test for significance\n",
    "        for approach_A, approach_B in approach_pairs:\n",
    "            scores_A = approach_prediction_splits[approach_A]\n",
    "            scores_B = approach_prediction_splits[approach_B]\n",
    "\n",
    "            if not np.mean(scores_A) > np.mean(scores_B):\n",
    "                continue\n",
    "            # =======================================\n",
    "            # Check for nomality\n",
    "            score_differences = [a - b for a, b in zip(scores_A, scores_B)]\n",
    "\n",
    "            # If normality > alpha, null hypothesis that test is normally distributed can be rejected\n",
    "            normality = shapiro(score_differences)[1]\n",
    "\n",
    "            # =======================================\n",
    "            # Test for significance, with test depending on normality\n",
    "            if normality > alpha:\n",
    "                # print(\"Normality test is significant. Running t-test.\")\n",
    "                # two sided t-test\n",
    "                # Passing the scores of the baseline first, to ensure testing the correct hypothesis\n",
    "                t_results = ttest_ind(scores_B, scores_A)\n",
    "                # correct for one sided test, according to Hitchhiker's guide\n",
    "                p_value = t_results[1] / 2\n",
    "\n",
    "                if p_value <= alpha:\n",
    "                    print(\n",
    "                        f\"{approach_A} is significantly better than {approach_B} with p-value {p_value:.4f} (t-test)\"\n",
    "                    )\n",
    "                    results[f\"{approach_A}___{approach_B}\"] = (\n",
    "                        f\"Significant with {p_value:.4f} (t-test)\"\n",
    "                    )\n",
    "                else:\n",
    "                    results[f\"{approach_A}___{approach_B}\"] = (\n",
    "                        f\"Not significant with {p_value:.4f} (t-test)\"\n",
    "                    )\n",
    "            else:\n",
    "                # print(\"Normality test not significant. Ommitting test for now.\")\n",
    "                results[f\"{approach_A}___{approach_B}\"] = \"No normal distribution\"\n",
    "                # We can use the wilcoxon-signed rank test when normality is not given, as it is\n",
    "                # non-parametric and thus does not make assumptions about the distribution\n",
    "                w_results = wilcoxon(scores_A, scores_B)\n",
    "                p_value = w_results[1]\n",
    "\n",
    "                if p_value <= alpha:\n",
    "                    print(\n",
    "                        f\"{approach_A} is significantly better than {approach_B} with p-value {p_value:.4f} (t-test).\"\n",
    "                    )\n",
    "                    results[f\"{approach_A}___{approach_B}\"] = (\n",
    "                        f\"Significant with {p_value:.4f} (wilcoxon)\"\n",
    "                    )\n",
    "                else:\n",
    "                    results[f\"{approach_A}___{approach_B}\"] = (\n",
    "                        f\"Not significant with {p_value:.4f} (wilcoxon)\"\n",
    "                    )\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For subsampled test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_predictions_subsampled = predictions_subsampled.sample(frac=1)\n",
    "twitteraae_test_subsampled_splits = np.array_split(\n",
    "    shuffled_predictions_subsampled, n_splits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approaches = [\"approach_finetuned\", \"approach_finetuned_subsample\"]\n",
    "baselines = [\"baseline_twitteraae\"]\n",
    "\n",
    "for score_index in [0, 1, 2]:\n",
    "    if score_index == 0:\n",
    "        print(\"\\n\\n\\n\")\n",
    "        print(\"Precision\")\n",
    "    elif score_index == 1:\n",
    "        print(\"\\n\\n\\n\")\n",
    "        print(\"Recall\")\n",
    "    elif score_index == 2:\n",
    "        print(\"\\n\\n\\n\")\n",
    "        print(\"F1 score\")\n",
    "\n",
    "    for pos_label in [0, 1, -1]:\n",
    "        if pos_label == 0:\n",
    "            print(\"Negative label\")\n",
    "        elif pos_label == 1:\n",
    "            print(\"Positive label\")\n",
    "        elif pos_label == -1:\n",
    "            print(\"Macro averaged\")\n",
    "\n",
    "        approach_prediction_splits = {}\n",
    "        for approach in [*approaches, *baselines]:\n",
    "            if pos_label == -1:\n",
    "                split_scores = [\n",
    "                    precision_recall_fscore_support(\n",
    "                        y_pred=split[f\"prediction_{approach}\"],\n",
    "                        y_true=split[\"aae_dialect_label\"],\n",
    "                        average=\"macro\",\n",
    "                    )[score_index]\n",
    "                    for split in twitteraae_test_subsampled_splits\n",
    "                ]\n",
    "            else:\n",
    "                split_scores = [\n",
    "                    precision_recall_fscore_support(\n",
    "                        y_pred=split[f\"prediction_{approach}\"],\n",
    "                        y_true=split[\"aae_dialect_label\"],\n",
    "                        average=\"binary\",\n",
    "                        pos_label=pos_label,\n",
    "                    )[score_index]\n",
    "                    for split in twitteraae_test_subsampled_splits\n",
    "                ]\n",
    "            approach_prediction_splits[approach] = split_scores\n",
    "\n",
    "        approach_pairs = [(x, y) for x in approaches for y in baselines]\n",
    "        results = {}\n",
    "        # Test for significance\n",
    "        for approach_A, approach_B in approach_pairs:\n",
    "            scores_A = approach_prediction_splits[approach_A]\n",
    "            scores_B = approach_prediction_splits[approach_B]\n",
    "\n",
    "            if not np.mean(scores_A) > np.mean(scores_B):\n",
    "                continue\n",
    "            # =======================================\n",
    "            # Check for nomality\n",
    "            score_differences = [a - b for a, b in zip(scores_A, scores_B)]\n",
    "\n",
    "            # If normality > alpha, null hypothesis that test is normally distributed can be rejected\n",
    "            normality = shapiro(score_differences)[1]\n",
    "\n",
    "            # =======================================\n",
    "            # Test for significance, with test depending on normality\n",
    "            if normality > alpha:\n",
    "                # print(\"Normality test is significant. Running t-test.\")\n",
    "                # two sided t-test\n",
    "                # Passing the scores of the baseline first, to ensure testing the correct hypothesis\n",
    "                t_results = ttest_ind(scores_B, scores_A)\n",
    "                # correct for one sided test, according to Hitchhiker's guide\n",
    "                p_value = t_results[1] / 2\n",
    "\n",
    "                if p_value <= alpha:\n",
    "                    print(\n",
    "                        f\"{approach_A} is significantly better than {approach_B} with p-value {p_value:.4f} (t-test)\"\n",
    "                    )\n",
    "                    results[f\"{approach_A}___{approach_B}\"] = (\n",
    "                        f\"Significant with {p_value:.4f} (t-test)\"\n",
    "                    )\n",
    "                else:\n",
    "                    results[f\"{approach_A}___{approach_B}\"] = (\n",
    "                        f\"Not significant with {p_value:.4f} (t-test)\"\n",
    "                    )\n",
    "            else:\n",
    "                # print(\"Normality test not significant. Ommitting test for now.\")\n",
    "                results[f\"{approach_A}___{approach_B}\"] = \"No normal distribution\"\n",
    "                # We can use the wilcoxon-signed rank test when normality is not given, as it is\n",
    "                # non-parametric and thus does not make assumptions about the distribution\n",
    "                w_results = wilcoxon(scores_A, scores_B)\n",
    "                p_value = w_results[1]\n",
    "\n",
    "                if p_value <= alpha:\n",
    "                    print(\n",
    "                        f\"{approach_A} is significantly better than {approach_B} with p-value {p_value:.4f} (t-test).\"\n",
    "                    )\n",
    "                    results[f\"{approach_A}___{approach_B}\"] = (\n",
    "                        f\"Significant with {p_value:.4f} (wilcoxon)\"\n",
    "                    )\n",
    "                else:\n",
    "                    results[f\"{approach_A}___{approach_B}\"] = (\n",
    "                        f\"Not significant with {p_value:.4f} (wilcoxon)\"\n",
    "                    )\n",
    "        print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialect-bias-detection-XNYuKIKE-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
